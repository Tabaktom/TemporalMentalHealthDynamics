{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Labels and Concatenate Clean samples and create sample representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre/control\n",
      "      index                       Date    Username                   ID  \\\n",
      "0         0  2019-11-29 22:53:32+00:00     GeoffSp  1200548374686113794   \n",
      "1         1  2019-11-24 12:36:55+00:00     GeoffSp  1198581260865675264   \n",
      "2         2  2019-11-23 20:55:06+00:00     GeoffSp  1198344245272289282   \n",
      "3         3  2019-11-21 20:47:27+00:00     GeoffSp  1197617541046247424   \n",
      "4         4  2019-11-19 22:06:51+00:00     GeoffSp  1196912748942286848   \n",
      "...     ...                        ...         ...                  ...   \n",
      "1824   1825  2014-01-02 23:18:43+00:00  Debtheblue   418884087102074880   \n",
      "1825   1826  2014-01-02 12:36:26+00:00  Debtheblue   418722451116548096   \n",
      "1826   1827  2014-01-02 11:03:55+00:00  Debtheblue   418699170644770816   \n",
      "1827   1828  2014-01-01 19:09:58+00:00  Debtheblue   418459100448837632   \n",
      "1828   1829  2014-01-01 17:12:09+00:00  Debtheblue   418429450804793344   \n",
      "\n",
      "                                                 CTweet  \n",
      "0     <S> ive just read the guardian article on how ...  \n",
      "1         <S> incorrect - go  stand in the corner <S/>   \n",
      "2     <S> rewatching rocketman what a great film mil...  \n",
      "3     <S> great day spent with outstanding london ce...  \n",
      "4              <S> we should form a support group <S/>   \n",
      "...                                                 ...  \n",
      "1824  <S> almost a year after seeing leighton baines...  \n",
      "1825      <S> hurrah table booked for us by phone <S/>   \n",
      "1826  <S> can we book a table for six people with wh...  \n",
      "1827  <S> you should add 'learn to use apostrophes o...  \n",
      "1828  <S> are you fully wheelchair accessible want t...  \n",
      "\n",
      "[3825994 rows x 5 columns]\n",
      "post/diagnosed\n",
      "      index                       Date        Username                   ID  \\\n",
      "0         0  2019-11-29 22:53:32+00:00         GeoffSp  1200548374686113794   \n",
      "1         1  2019-11-24 12:36:55+00:00         GeoffSp  1198581260865675264   \n",
      "2         2  2019-11-23 20:55:06+00:00         GeoffSp  1198344245272289282   \n",
      "3         3  2019-11-21 20:47:27+00:00         GeoffSp  1197617541046247424   \n",
      "4         4  2019-11-19 22:06:51+00:00         GeoffSp  1196912748942286848   \n",
      "...     ...                        ...             ...                  ...   \n",
      "3818   3849  2014-01-04 16:52:45+00:00  naushabah_khan   419511730129993729   \n",
      "3819   3850  2014-01-04 16:27:30+00:00  naushabah_khan   419505375256145921   \n",
      "3820   3851  2014-01-04 16:18:16+00:00  naushabah_khan   419503052324745216   \n",
      "3821   3852  2014-01-04 12:52:02+00:00  naushabah_khan   419451152686919680   \n",
      "3822   3853  2014-01-03 15:34:10+00:00  naushabah_khan   419129566758567936   \n",
      "\n",
      "                                                 CTweet  \n",
      "0     <S> ive just read the guardian article on how ...  \n",
      "1         <S> incorrect - go  stand in the corner <S/>   \n",
      "2     <S> rewatching rocketman what a great film mil...  \n",
      "3     <S> great day spent with outstanding london ce...  \n",
      "4              <S> we should form a support group <S/>   \n",
      "...                                                 ...  \n",
      "3818  <S> supporting small businesses with local can...  \n",
      "3819  <S> our candidate supporting the savestroodlib...  \n",
      "3820  <S> disappointed to see that robin thicke awfu...  \n",
      "3821  <S> “: drying off after a very wet labour door...  \n",
      "3822  <S> excellent visit to strood community projec...  \n",
      "\n",
      "[3319201 rows x 5 columns]\n",
      "control 1365\n",
      "                             Date        Username  \\\n",
      "0       2019-11-29 22:53:32+00:00         GeoffSp   \n",
      "1       2019-11-24 12:36:55+00:00         GeoffSp   \n",
      "2       2019-11-23 20:55:06+00:00         GeoffSp   \n",
      "3       2019-11-21 20:47:27+00:00         GeoffSp   \n",
      "4       2019-11-19 22:06:51+00:00         GeoffSp   \n",
      "...                           ...             ...   \n",
      "3319196 2014-01-04 16:52:45+00:00  naushabah_khan   \n",
      "3319197 2014-01-04 16:27:30+00:00  naushabah_khan   \n",
      "3319198 2014-01-04 16:18:16+00:00  naushabah_khan   \n",
      "3319199 2014-01-04 12:52:02+00:00  naushabah_khan   \n",
      "3319200 2014-01-03 15:34:10+00:00  naushabah_khan   \n",
      "\n",
      "                                                    CTweet  \n",
      "0        <S> ive just read the guardian article on how ...  \n",
      "1            <S> incorrect - go  stand in the corner <S/>   \n",
      "2        <S> rewatching rocketman what a great film mil...  \n",
      "3        <S> great day spent with outstanding london ce...  \n",
      "4                 <S> we should form a support group <S/>   \n",
      "...                                                    ...  \n",
      "3319196  <S> supporting small businesses with local can...  \n",
      "3319197  <S> our candidate supporting the savestroodlib...  \n",
      "3319198  <S> disappointed to see that robin thicke awfu...  \n",
      "3319199  <S> “: drying off after a very wet labour door...  \n",
      "3319200  <S> excellent visit to strood community projec...  \n",
      "\n",
      "[3319201 rows x 3 columns]\n",
      "Depressed    289624\n",
      "None         289624\n",
      "Name: Label, dtype: int64\n",
      "235\n",
      "                             Date         Username  \\\n",
      "0       2019-11-29 11:23:17+00:00  ItsLaurenYvonne   \n",
      "1       2019-11-29 10:57:49+00:00  ItsLaurenYvonne   \n",
      "2       2019-11-29 07:03:32+00:00  ItsLaurenYvonne   \n",
      "3       2019-11-28 23:15:45+00:00  ItsLaurenYvonne   \n",
      "4       2019-11-28 20:10:12+00:00  ItsLaurenYvonne   \n",
      "...                           ...              ...   \n",
      "289619  2019-11-23 06:54:22+00:00          bolu_ay   \n",
      "289620  2019-11-23 06:47:29+00:00          bolu_ay   \n",
      "289621  2019-11-23 06:47:02+00:00          bolu_ay   \n",
      "289622  2019-11-23 06:41:45+00:00          bolu_ay   \n",
      "289623  2019-11-23 06:19:26+00:00          bolu_ay   \n",
      "\n",
      "                                                   CTweet      Label  \n",
      "0       <S> oh my god myfavoritemurder were doing a li...  Depressed  \n",
      "1                                   <S> it so great <S/>   Depressed  \n",
      "2       <S> it a rollercoaster mate brilliantly writte...  Depressed  \n",
      "3       <S> i just watched a simple favour and ohhh ma...  Depressed  \n",
      "4                             <S> we love to see it <S/>   Depressed  \n",
      "...                                                   ...        ...  \n",
      "289619  <S> freethrow shooting fallen off ( used to be...       None  \n",
      "289620             <S> itll tighten in the playoffs <S/>        None  \n",
      "289621  <S> the west isnt gonna be a slugfest that peo...       None  \n",
      "289622      <S> you think eastern teams are better  <S/>        None  \n",
      "289623       <S> weve all seen this movie before yo <S/>        None  \n",
      "\n",
      "[579248 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "#cnt_path =r'/Users/tom/Desktop/France_tweets/experiment/pre_clean/'\n",
    "#dep_path = r'/Users/tom/Desktop/France_tweets/experiment/during_clean/'\n",
    "\n",
    "#cnt_path=r'/Users/tom/Desktop/germany_tweets/development/control_clean/'\n",
    "#dep_path =r'/Users/tom/Desktop/germany_tweets/development/diagnosed_clean/'\n",
    "\n",
    "#cnt_path =r'/Users/tom/Desktop/germany_tweets/experiment/all_clean/'\n",
    "\n",
    "#cnt_path =r'/Users/tom/Desktop/France_tweets/development/control_group_clean/'\n",
    "#dep_path =r'/Users/tom/Desktop/France_tweets/development/diagnosed_group_clean/'\n",
    "cnt_path = r'/Users/tom/Desktop/MSc Thesis/Data/UK_tweets/Development_Depression_Country_A/new_control_group_clean_tweets/'\n",
    "dep_path = r'/Users/tom/Desktop/MSc Thesis/Data/UK_tweets/Development_Depression_Country_A/new_depression_clean_clean/'\n",
    "\n",
    "cnt_data = {fname: pd.read_csv(cnt_path + fname, index_col=0, parse_dates=True) for fname in os.listdir(cnt_path)}\n",
    "dep_data = {fname: pd.read_csv(dep_path + fname, index_col=0, parse_dates=True) for fname in os.listdir(dep_path)}\n",
    "cnt_df = pd.concat(cnt_data.values())\n",
    "dep_df = pd.concat(dep_data.values())\n",
    "print('pre/control')\n",
    "print(cnt_df)\n",
    "cnt_df=cnt_df.drop_duplicates()\n",
    "print('post/diagnosed')\n",
    "print(cnt_df)\n",
    "cnt_df = (cnt_df.reset_index(drop=True)).drop(columns=['index', 'ID'])\n",
    "dep_df = (dep_df.reset_index(drop=True)).drop(columns=['index', 'ID'])\n",
    "cnt_df['Date'] = pd.to_datetime(cnt_df['Date'])\n",
    "#dep_df['Date'] = pd.to_datetime(dep_df['Date'])\n",
    "\n",
    "#dep_df = dep_df.groupby(['Username']).sum()\n",
    "#cnt_df=cnt_df.groupby(['Username']).sum()\n",
    "\n",
    "# Sample representation\n",
    "#dep_df = dep_df.groupby(['Username', pd.Grouper(key='Date', freq='W')]).sum()\n",
    "#cnt_df = cnt_df.groupby(['Username', pd.Grouper(key='Date', freq='W')]).sum()\n",
    "\n",
    "print('control', len(list(set(cnt_df['Username'].values.tolist()))))\n",
    "print(cnt_df)\n",
    "\n",
    "\n",
    "#print('depression', len(list(set(dep_df['Username'].values.tolist()))))\n",
    "#print(dep_df)\n",
    "\n",
    "\n",
    "dep_df['Label'] = pd.Series(['Depressed']*len(dep_df.index), index=dep_df.index)\n",
    "cnt_df['Label'] = pd.Series(['None']*len(cnt_df.index),index=cnt_df.index)\n",
    "#full = pd.concat([dep_df, cnt_df])\n",
    "\n",
    "full = pd.concat([dep_df, cnt_df])\n",
    "print(full['Label'].value_counts())\n",
    "print(len(list(set(full.Username))))\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n",
      "                             Date         Username  \\\n",
      "0       2017-11-13 22:03:59+00:00     matthewpryke   \n",
      "1       2014-01-20 21:44:37+00:00      1donethingx   \n",
      "2       2018-01-20 10:08:22+00:00         Jezza182   \n",
      "3       2019-09-26 09:17:52+00:00  musicandmontage   \n",
      "4       2019-10-09 07:36:00+00:00        lynnannem   \n",
      "...                           ...              ...   \n",
      "579243  2017-12-18 13:22:29+00:00    Derek_duPreez   \n",
      "579244  2016-06-28 22:29:48+00:00    AdrienneKlasa   \n",
      "579245  2018-05-20 17:08:06+00:00   lovebillybragg   \n",
      "579246  2019-08-22 08:56:30+00:00       hairydalek   \n",
      "579247  2014-12-10 09:32:07+00:00     matthewpryke   \n",
      "\n",
      "                                                   CTweet      Label  \n",
      "0       <S> maro itoje amp owen farrell england pair n...       None  \n",
      "1       <S> best part of the show was the foxes to cut...  Depressed  \n",
      "2       <S> before travis was officially signed in as ...  Depressed  \n",
      "3       <S> if you were nervous you didnt seem it at a...       None  \n",
      "4       <S> appreciate this i cannot get the hang of i...  Depressed  \n",
      "...                                                   ...        ...  \n",
      "579243  <S> agh so horrible feeling like that hope you...       None  \n",
      "579244  <S> more terrible news attack at istanbul airp...       None  \n",
      "579245  <S> hi thank all your support over the past fe...  Depressed  \n",
      "579246                              <S> old and new <S/>   Depressed  \n",
      "579247  <S> incredible strength of character as austra...       None  \n",
      "\n",
      "[579248 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "full_1 = full.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#full_2 = full_test.sample(frac=1).reset_index(drop=True)\n",
    "#full_1['Cnt'] = pd.Series([1]*len(full_1.index), index=full_1.index)\n",
    "#full_1 = full_1.groupby(['Username',pd.Grouper(key='Date', freq='D')]).sum()\n",
    "#train_df = full_1[: int(len(full_1.index)*0.2)].groupby(['Username', pd.Grouper(key='Date', freq='W')]).sum()\n",
    "\n",
    "\n",
    "train = full_1[:int(len(full_1.index)*0.8)]\n",
    "test = full_1[int(len(full_1.index)*0.8):]\n",
    "#print(np.unique(train.Label.values))\n",
    "#train.to_csv(r'/Users/tom/Desktop/Country_A_tweets/Development_Depression_Country_A/train_user_day.csv', encoding = 'utf-8')\n",
    "###full_1.to_csv(r'/Users/tom/Desktop/Country_A_tweets/Experiment_depression_Country_A/exp_indiv.csv', encoding='utf-8')\n",
    "\n",
    "train.to_csv('/Users/tom/Desktop/UK_train_indiv.csv', encoding='utf-8')\n",
    "test.to_csv('/Users/tom/Desktop/UK_test_indiv.csv', encoding='utf-8')\n",
    "#full_2.to_csv(r'/Users/tom/Desktop/indiv_test.csv', encoding='utf-8')\n",
    "#full_1.to_csv(r'/Users/tom/Desktop/UK_balanced.csv', encoding='utf -8')\n",
    "#full_1.to_csv(r'/Users/tom/Desktop/spain_tweets/experiment/user_day_experiment.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
